# Syntax Analysis(Parser)

A Syntax analyzer is formally defined as :

> An Algorithm that Groups the Set of Tokens Sent by the Scanner to Form
> **Syntax Structures** Such As Expressions, Statements, Blocks,etc.

Simply put, the parser examines if the source code written follows
the grammar(production rules) of the language.


The Syntax structure of programming languages and even spoken languages
can be expressed in what is called **BNF** notation, which stands 
for **B**akus **N**aur **F**orm. 

For example, in spoken English, we can say the following:

> sentence --> noun-phrase	verb-phrase
>
> noun-phrase --> article	noun 
>
> article --> THE | A | ...
> 
> noun --> STUDENT | BOOK | ...
>
> verb-phrase --> verb noun-phrase 
>
> verb --> READS | BUYS | ....

Note : The BNF Notation uses [different symbols](https://en.wikipedia.org/wiki/Backus%E2%80%93Naur_form#Example),
for example, a sentence is defined as :

> \< sentence \> ::= \< noun-phrase \>	\< verb-phrase \>

But this is very cumbersome, so we use the first notation, since its
easier to use. 

Now, let us derive a sentence : 

> sentence --> **noun-phrase** verb-phrase 
>
> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -->
> **article** noun verb-phrase
>
> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -->
> THE **noun** verb-phrase
>
> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -->
> THE STUDENT **verb-phrase**
>
> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -->
> THE STUDENT **verb** noun-phrase
>
> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -->
> THE STUDENT READS **noun-phrase**
> 
> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -->
> THE STUDENT READS **article** noun
>
> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -->
> THE STUDENT READS A **noun**
>
> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -->
> THE STUDENT READS A BOOK


In the same way, the parser tries to **derive** your source program 
from the starting symbol of the grammar.

Lets say we have these sentences :

> THE BOOK BUYS A STUDENT
>
> THE BOOK WRITES A DISH
>
> THE DISH TAKES A STROLL

Syntax-wise, all of these sentences are correct. However, their meaning 
is not correct, and they are not useful. What differentiates 2
sentences that are grammatically correct is their meaning or their 
**semantics**. You and I can agree that the meaning of a grammatically 
correct sentence is not correct, but how does the computer do it?



## Grammar 

> A grammar G=(V<sub>N</sub>, V<sub>T</sub>, S, P) where:
>
> 1. V<sub>N</sub> : A finite set of nonterminals(nonterminals set).
> 2. V<sub>T</sub> : A finite set of terminals(terminals set).
> 3. S &isin; V<sub>N</sub> : The Starting symbol of the grammar. 
> 4. P =  A set of **production rules**(productions).<-- Pending <==> Basically the whole grammar.

Note :

1. V<sub>N</sub> &cap; V<sub>T</sub> = &empty;.
2. V<sub>N</sub> &cup; V<sub>T</sub> = V(the vocabulary of the grammar).

Note : We will use 

1. Uppercase Letters A,B,...,Z for non-terminals.

2. Lowercase Letters a,b,...,z for terminals.

3. Greek letters &alpha;,&beta;,&gamma;,... for strings formed from V<sub>N</sub> OR V<sub>T</sub> = V. eg, 

   if 
   
   > V<sub>N</sub> = {S,A,B}
   
   and 
   
   > V<sub>T</sub> = {0,1}
   
   then
   
   > &alpha; = 0A11B
   > 
   > &beta; = S110B
   >
   > &gamma; = 0010

### Productions 

1. A Production &alpha; --> &beta;(alpha derives beta) is a rewriting rule such that
the occurrence of &alpha; can be substituted by &beta; in any string.

   Note that &alpha; must contain at least one nonterminal from,&isin;V<sub>N</sub>. 

   For example, Assume we have the string &gamma;&alpha;&sigma;,

   > &gamma;&alpha;&sigma; --> &gamma;&beta;&sigma;
   
2. A Derivation is a sequence of strings &alpha;<sub>0</sub>, &alpha;<sub>1</sub>,
&alpha;<sub>2</sub>, &alpha;<sub>3</sub>,....,&alpha;<sub>n</sub>, then :

	- &alpha;<sub>0</sub> -*-> &alpha;<sub>n</sub>, n &ge; 0.
	
	- &alpha;<sub>0</sub> -<sup>+</sup>-> &alpha;<sub>n</sub>, n &ge; 1.
	

Given a grammar G, then :

> L(G) = Language Generated By the Grammar.
	
for example, Given the Grammar, G = ({S,B,C},{a,b,c},S,P)

P :

> S --> aSBC 
>
> S --> abC
>
> CB --> BC
>
> bB --> bb
>
> bC --> bc
>
> cC --> CC

L(G)=?

Lets follow through on the derivations

> S --> a**bC** --> abc(all terminals) &isin; L(G) <--- A sentence
>
> S --> a**S**BC --> aa**bC**BC --> aabbcBC --> blocked, so we try another path
> 
> S --> a**S**BC --> aab**CB**C --> aa**bB**CC --> aab**bC**C --> aabb**cC** --> aabbcc &isin; L(G) <--- A sentence
>
> S --> a**S**BC -->........-->aaabbbccc &isin; L(G) <--- A sentence
>
> Therefore, L(G)={a<sup>n</sup>,b<sup>n</sup>,c<sup>n</sup>| n &ge; 1}


As another Example, we have these productions

> E --> E+T <-- we can write the productions 1 and 2 as a single production E --> E+T | T
>
> E --> T
>
> T --> T*F
>
> T --> F
>
> F --> (E) <-- we can write the productions 5 and 6 as a single production F --> (E) | n
>
> F --> n

Lets follow through some derivations

> E --> **T** --> **F** --> n &isin; L(E)
>
> E --> **E**+T --> T+**T** --> T+**F** --> **T**+n --> **F**+n --> n+n &isin; L(E)
>
> E --> **E**+T --> **T**+T --> **F**+T --> n+**T** --> n+**F** --> n + (**E**) --> n+(**T**)
> --> n+(**T**\*F) --> n+(**F**\*F) --> n+(n\***F**) --> n+(n\*n) &isin; L(E)
>
> Therefore, L(G) = {Any arithmetic expression with \* and + operations},
> n is an operand here.

Note that, if we add the productions

> E --> E+T | E-T | T
>
> T --> T\*F | T/F | T%F

We would have a language to express all arithmetic expressions with 
(\*,\\,+,\-) operations.

Lets Take another Example(things in double quotes are terminals)

> Program --> block "#"
>
> block --> "{" stmt-List "}"
>
> stmt-List --> statement ";" stmt-List | &lambda;
>
> statement --> if-stmt | while-stmt | read-stmt | write-stmt |
> assignment-stmt | block
>
> if-stmt --> "if" condition.... 
>
> while-stmt --> "while" condition.....
>
> ....
> 
> ....
> 
> read-stmt --> "read"
>
> write-stmt --> "write"


V<sub>N</sub> = {Program, block, stmt-List, statement, if-stmt,
while-stmt, read-stmt, write-stmt, assignment-stmt}

V<sub>T</sub> = { "{", "}", "#", ";", "if", "while", "read", "write" }


Lets Follow through some derivations :

> Program --> **block** # --> { **stmt-list** } # --> { &lambda; } #
>
> Program --> **block** # --> {**stmt-list**} # --> {statement ; **stmt-list**} #
> --> {statement ; statement ; **stmt-list**} # --> {statement ; statement ; &lambda;} #
> --> {**statement** ; statement ;} # -->  {**READ-statement** ; statement ;} #
> -->{READ ; **statement** ;} # -->{READ ; write-statement ;} # --> {READ ; WRITE ;} #

We can write this as 

```
{ READ;
  WRITE;
}#
```
The language of this language is defined as 

> L(G) = {Set of all programs that can be written in this language}.

This is only a simple example, of a simple language. For something more
complex such as C or Pascal, there are hundreds of productions.


### Algorithms for Derivation 


>A Leftmost derivation is a derivation in which we replace the **leftmost**
>nonterminal in each derivation step.


>A Rightmost derivation is a derivation in which we replace the **rightmost**
>nonterminal in each derivation step.

For example, given the grammar

>V --> S R $
>
>S --> +|-|&lambda;
>
>R --> .dN | dN.N
>
>N --> dN | &lambda;
>
>V<sub>N</sub> = {V,R,S,N}
>
>V<sub>T</sub> = {+, - , ., d, $}

Lets follow through on the leftmost derivation 

> V --> **S**R$ --> -**R**$ --> -d**N**.N$ --> -dd**N**.N$ --> -dddN.N$
> --> -ddd.**N**$ --> -ddd.d**N**$ --> -ddd.d$ <-- A sentence.

Lets follow through on the rightmost derivation

> V --> S**R**$ --> SdN.**N** --> SdN.d**N**$ --> Sd**N**.d$ --> sdd**N**.d$
> --> sddd**N**.d$ --> **S**ddd.d$ --> -ddd.d$ <-- A sentence.

### Derivation Trees

A Derivation Tree is a Tree that displays the derivation of some 
sentence in the language. For example, lets look at the 
tree for the previous example

![ddd](./images/derivationtree.PNG)

Note that if we traverse the tree in order, recording **only** the leaves,
we obtain the sentence.

### Classes of Grammars 

According to Chomsky, There are 4 classes of grammars :

1. Unrestricted Grammars : No restrictions whatsoever except the restriction
by definition that the left side of the production contains at least one
nonterminal from V<sub>s</sub>. This grammar is not practical and we cannot
work with it. 

2. Context-Sensitive Grammars : For each production &alpha; --> &beta;,
|&alpha;| &le; |&beta;|, ie , the **length of alpha(&alpha;)** is less than or equal to 
the **length of Beta(&beta;)**. This means that in this class of grammar, there are no
&lambda; productions in the form A --> &lambda;, since |&lambda;| = 0 and A &ge; 1.

   They Say that Fortran has a context-sensitive grammar.
   
   It is very difficult to work with this class of grammars.

3. Context-Free Grammar(CFG) : Each production in this grammar class is of the 
form A --> &alpha; , where A &isin; V<sub>N</sub> and &alpha; &isin; V<sub><sub>*</sub></sub>
that is to say, the left hand side is **only** one nonterminal.

   This is the most important class of grammar. Most programming languages's
   structures are context-free. 
   
   We will mostly be working with this class of grammar. Most of the
   examples we have taken are CFG.

4. Regular Grammar (Regular Expressions) : Each production in this grammar class
   is of the form A -- > aB or A --> a, where A,B &isin; V<sub>N</sub> and a &isin;
   V<sub>T</sub>, **with the exception** of S --> &lambda;
   
   For example, lets say we have the grammar
   
   > A --> aA
   >
   > A --> a
   >
   Therefore, we get 
   
   >
   >G(L)=a<sup>+</sup>
   > 
	
   However, adding the production 
   
   >
   > A --> &lambda;
   >
   
   Results in the grammar
   
   >
   > G(L)=a\*
   > 

## Parsing Techniques

There are 2 main parsing techniques used by a compiler.

### Top-Down Parsing

In Top-Down Parsing, the parser builds the derivation tree from the 
root(S : the starting symbol) down to the leaves(sentence). 

In Simple words, the parser tries to derive the sentence using 
leftmost derivation. For example, say we have this grammar :

> V --> SR$
>
> S --> + | - | &lambda;
>
> R --> .dN | dN.N
>
> N --> dN | &lambda;

Lets examine if the sentence

> dd.d$ 

is derived from this grammar.

> V --> **S**R$ --> +**R**$ --> d**N**.N$ --> dd**N**.N$ --> dd.**N**$ --> dd.d**N**$ --> dd.d$

Therefore, this sentence is derived from the grammar.

However, this approach is very computationally intensive, and more importantly, 
this requires knowing the source code in advance. The Parser doesnt know
which production it should select in each derivation statement. We will
learn how to solve these issues later in the course.

### Bottom-Up Parsing

In Bottom-Up Parsing, the parser builds the derivation tree from the
leaves(sentence) up to the root(S : Starting Symbol). This type of tree,
built from the leaves to the root, is 
called a [B-Tree](https://en.wikipedia.org/wiki/B-tree).

In Simple words, the parser starts with the given sentence, does 
**reduction**(opposite of derivation) steps, until the starting symbol
is reached.

Note that the string &lambda; is present everywhere in the string, and
we can use it wherever we like.

Lets follow the reduction of the example given above.

> +dd.d$ --> +dd&lambda;.d$ --> +ddN.d$ --> +dN.d$ --> +dN.d&lambda;$
> --> +dN.dN$ --> +dN.N$ --> +R$ --> SR$ --> V

Which means that the sentence is in the grammar. 

Note that we can run into deadlocks here. say we took this path instead :

> +dd.d$ --> +dd&lambda;.d$ --> +ddN.d$ --> +dN.d$ --> +dN.d&lambda;$
> --> +dN.dN$ --> **+dNR$ --> +NR$ --> SNR$** --> Deadlock

This technique also has a major problem : Which substring should we 
select to reduce in each reduction step?

how do we solve this?

## Ambiguity 

Given the following grammar :

> num --> num d
>
> num --> d

Let us draw the derivation tree for the sentence ```dddd```

![numd](./images/num_d.PNG)

Question : is there another derivation tree that represents the sentence?

The answer is **no**.

If there is only one derivation tree representing the sentence, 
this means there is only one way to derive the sentence.

Based on this, we can say that :

> A Grammar G is said to be ambiguous if there is one sentence with more than
> one derivation tree. 
> 
> That is, there is more than one way to derive the sentence. 
>
> This means that our algorithm is **non-deterministic**.

Say we have this grammar

> E --> E + E
>
> E --> E * E
>
> E --> (E) | a

Take the sentence :

> a + a * a

Lets draw the derivation tree

![aaa](./images/left.PNG)


Due to the fact that we have 2 trees that give the same result, we can 
say that this grammar is ambiguous.

In this case, to enforce the associativity rule, this grammar can 
be re-written as :

> E --> E + E | T 
>
> T --> T*T | F
>
> F--> (E) | a

Now, Take the sentence ```a + a * a```
and find the derivation tree now. 
![left](./images/eft1.PNG)

There is only 1 possible derivation tree now. This solves the associativity
issue of the grammar before with the ```+``` and ```*``` operations.

But lets say we have the sentence :

> a + a + a

Lets try to find the derivation tree and any alternative trees. 

![eft1](./images/aaa..PNG)
![aaa2](./images/aaa2.PNG)

We can see here that there is more than 1 derivation tree, and the 
language is still ambiguous.

We can solve this if we rewrite the grammar with the **left-associative rule**

> E --> E + T | T
>
> T --> T * F | F
>
> F --> (E) | a

The resultant grammar is left-associative.

This grammar solves the problems of :

- ambiguity.
- precedence.
- associativity.

Lets try rewriting it with the **right-associative rule**

> E --> T + E | T
>
> T --> F * T | F
>
> F --> (E) | a

Lets try creating the derivation tree of ```a + a * a```

![right1](./images/right1.PNG)

Now lets draw the derivation tree of ```a + a + a```

![righ2](./images/right2.PNG)

This new grammar is not ambiguious, however, as we can tell from the derivation 
trees, there are precedence issues now. It's not technically wrong, 
but it doesnt not follow standard arithmetic rules.

Back to the left-associative grammar now. This grammar is called
**left-recursive**. This causes problems when it omes to top-down parsing
techniques(we will see why later).

A grammar is said to be left recursive if there is a production of the form:

> A-->A&alpha;

Conversely, a grammar is right-recursive if there is a production of the form:

> A-->&alpha;A

And causes no problems in top-down parsing.

our grammar has 2 rules of the form 

>A-->A&alpha;

The solution is to transform the grammar to a grammar which is not 
left-recursive. 

This has an algorithm to it. 

Given that 

> A-->A&alpha;<sub>1</sub>| A&alpha;<sub>2</sub>|A&alpha;<sub>3</sub>|...|A&alpha;<sub>n</sub>
>
> A-->&beta;<sub>1</sub>|&beta;<sub>2</sub>|&beta;<sub>3</sub>|...|&beta;<sub>m</sub>

To do this, we must introduce a new non-terminal, say A\`.

The grammar now becomes :

> A-->&beta;<sub>1</sub>A\`|&beta;<sub>2</sub>A\`|&beta;<sub>3</sub>A\`|...|&beta;<sub>m</sub>A\`

and

> A\`-->&alpha;<sub>1</sub>A\`| &alpha;<sub>2</sub>A\`|&alpha;<sub>3</sub>A\`|...|&alpha;<sub>n</sub>A\`|&lambda;

For example, say we have

>A-->Ab
>
>A-->a
>
>L(G)=ab*

Then according to the above

>A-->aA\`
>
>A\`-->bA\`|&lambda;

which results in the same grammar.

Lets apply this to the grammar :

> E --> E + T | T
>
> T --> T * F | F
>
> F --> (E) | a

This results in :

> E --> T E\` 
>
> E\` --> + T E\` | &lambda;
>
> T --> F T\`
>
> T\` --> \* F T\` | &lambda;
>
> F --> (E) | a

This grammar is now **perfect**. It solves all our ambiguity issues, and this is a
grammar we can use to construct the production rules for our programming
language.

Another ambiguity in programming languages is the ```if...else```
statement.

Lets take a generic if statement in a generic language:

> stmt --> if-stmt | while-stmt | ....
>
> if-stmt --> IF condition stmt
>
> if-stmt --> IF condition stmt ELSE stmt 
>
> condition --> C
>
> stmt --> S 

This grammar is ambiguous.

Lets take the nested ```if...else``` statement :

```
IF C
	IF C
		S
	ELSE
		S
		
```
This statement results in 2 derivations trees.

Both these trees result in the same traversal, but they have different meanings.
The first results in the ```ELSE``` belonging to the first ```IF```, while the 
second results int he ```ELSE``` belonging to the second ```IF```. We as 
humans know that the ```ELSE``` belongs to the second ```IF```, since we know that
the ```ELSE``` statement follows the nearest ```IF```. but how can 
the compiler know?

There are a bunch of solutions to this problem:

1. Add a delimiter to the ```IF``` statement, such as ```ENDIF``` or 
```END``` or ```FI``` to the end of the statement, resulting in these 
productions :
> if-stmt --> IF condition stmt **ENDIF**
>
> if-stmt --> IF condition stmt ELSE stmt **ENDIF**

   Resulting in this statement :
	
   ```
   IF C
   |	IF C
   |	|	S
   |	|ELSE
   |	|	S
   |	ENDIF
   ENDIF
	
   ```
	
   The grammar is now unambigious, since we have to clearly state when 
   an ```IF``` statement ends. However, this is not a pretty solution, 
   and is extra work for both the programmar and compiler, and results
   in less readable code.
	
2. In C and Pascal, the compiler **always** prefers to shift the ```ELSE```
	when it sees it in the source code so it follows the nearest ```IF```. We 
	will learn about this in more detail later.
	
Another thing about this grammar is **left factoring**. 

### Left Factoring
Consider the productions :

> A --> &alpha;&beta;
>
> A --> &alpha;&gamma;

Note how the first part of the productions is the same. This grammar
can be transformed by introducing a new non-terminal, So what happens now is:

> A --> &alpha;B
>  
> B --> &beta;&gamma;

For our grammar, this results in 

> if-stmt --> IF condition stmt
>
> if-stmt --> IF condition stmt ELSE stmt 

becoming

> if-stmt --> IF conditon stmt else-part
>
> else-part --> ELSE stmt | &lambda;

Does this solve the ambiguity? No, but it helps in removing choices, since 
the if-stmt is now one production. If we look at the statement :


```
IF C
	IF C
		S
	ELSE
		S
		
```

It still has 2 derivation trees

## More Ways of Expressing Programming Languages

### Extended BNF Notation

So far, we have been using **BNF Notation**(Production rules) to express
languages. However, there is another form to Express a language, which is 
**Extended BNF Notation**

if there is repetition in the grammar, say in the example of the grammar


> E --> E + T | T
> 
> T --> T \* F | F
>
> F --> (E) | a

which can give us a derivation in the form of 

> E --> **E** + T --> **E** + T + T --> **E** + T + T + T --> T + T + T + T....+T

or in the same line, 

> T --> T * F --> T * F * F --> T * F * F * F --> T * F * F * F....* F

We can express this grammar as :

> E --> T { + T }
>
> T --> F { * F }
>
> F --> (E) | a

We know that \[x\] means that we take x 0 or 1 time only.

However, \{ x \} means we take x zero or any number of times. This is 
equivalent to \(x\)\*

We can also express this grammar as:

> E --> T (+ T)\*
>
> T --> F (* F)\*
>
> F --> (E) | a

### Syntax Diagrams

Another way to express languages are **Syntax Diagrams**. These are used
only with Extended-BNF notation.

in these diagrams, A square shape represents a nonterminal and an oval shape
represents a terminal. Lets take a look at the examples below :

![Syntax Diagrams](https://upload.wikimedia.org/wikipedia/commons/thumb/6/65/Example_syntax_diagram_3.svg/506px-Example_syntax_diagram_3.svg.png)

## Parsing Techniques (Continued)

Recall : The parser is an algorithm which accepts or rejects a sentence
in the programming language. 

Recall : There are 2 kinds of parsers : 

1. Top-Down Parsers : In This parsing technique, The parser starts with 
```S``` using leftmost derivation to derive the sentence. The Major problem
with this parsing technique is that the parser doesn't know which production
it should select in each derivation step.

2. Bottom-Up Parsers : The parser in this parsing technique starts 
from the sentence, doing reduction steps, until it reaches the starting
symbol ```S``` of the grammar. The Major problem with this technique
is that the parser doesn't know which substring the parser should select
in each reduction step.

In Top-Down parsing, we have 2 available algorithms for parsing :

1. Recursive Descent Parsing.
2. LL(1) Predictive Parsing.

In Bottom-Up parsing, we have 2 available algorithms for parsing :

1. LR Parsers.
2. Operator Precedence Parsers --> Uses matrix manipulation.

Before we continue, we need to define a few functions

### The FIRST() Function 

Given a string &alpha; &isin; V\*, then 

> FIRST(&alpha;) = { a | &alpha; --\*--> aw, a&isin;V<sub>T</sub>,w&isin;V\*}

in addition, if &alpha; --> &lambda;, then we add &lambda; to FIRST(&alpha;), that is

> &lambda; &isin; FIRST(&alpha;)

That is to say,  FIRST(&alpha;) = Set of all terminals that may begin 
strings derived from &alpha;.

For example 

> &alpha; --\*--> cBx
>
> &alpha; --\*--> ayD
>
> &alpha; --\*--> ab
>
> &alpha; -----> ddd

Then

> FIRST(&alpha;) = {c,a,d}

Assume as well that

> &alpha; --\*--> &lambda;

then 

> FIRST(&alpha;) = {c,a,d,&lambda;}

That is to say, **&lambda; appears in the FIRST() function**.

### The FOLLOW() Function

We define the FOLLOW() function for **only** nonterminals. That is 
to say 

> FOLLOW(A), A&isin;V<sub>N</sub>

Given 

> S --\*--> uA&beta; , u&isin;V<sub>T</sub>, A&isin;V<sub>N</sub>, &beta;&isin;V\*

then

> FOLLOW(A)=FIRST(&beta;)

That is to say, FOLLOW(A) =  The set of all terminals that may appear after A in 
the derivation.

> S --\*--> aaXdd
>
> S --\*--> Xa
>
> S --\*--> BXc

Then 

> FOLLOW(X) = {d,a,c}

### Rules To Compute FIRST() and FOLLOW() Sets

1. FIRST(&lambda;) = {&lambda;}.
2. FIRST(a) = { a }.
3. FIRST(a&alpha;)= {a}.
4. FIRST(XY) = FIRST(FIRST(X).FIRST(Y)) **OR** FIRST(X.FIRST(Y)) **OR** FIRST(FIRST(X).Y).
5. Given the production A --> &alpha;X&beta;, Then :

   a. FIRST(&beta;) &sub; FOLLOW(X) if &beta; &ne; &lambda;.
   
   b. FOLLOW(A) &sub; FOLLOW(X) if &beta; = &lambda;.

Note that the FIRST() and FOLLOW() sets are made of **terminals only**

By these rules, say we have 

> A -- > &alpha;X&beta; , X&isin;V<sub>N</sub>

and say we want FOLLOW(X)

Then 

> FIRST(&beta;) &sub; FOLLOW(X)

We say it is a subset because we can have other productions involving 
X.

Assuming that &beta; = &lambda;, Things are different. 

Say that we have a production that leads to this derivation is 

> S --*--> uA&gamma;

and following through this results in this derivation : 

> S --*--> uA&gamma; --> u&alpha;X&gamma;

Therefore, 

> FOLLOW(A) &sub; FOLLOW(X)

This is because whatever follows A can follow X if there is nothing 
between them.

Notes : 

1. &lambda; **may** appear in FIRST() but it doesn't appear in FOLLOW(). We 
will see this when we define augmented grammars.

2. Generally, we start computing the FIRST() from bottom to top, But follow
from top to bottom.

3. When we compute FOLLOW(X), we search for X in the right side of 
any production.

#### Augmented Grammars

Given the grammar G=(V<sub>N</sub>,V<sub>T</sub>,S,P), then the 
augmented grammar G\`=(V<sub>N</sub>\`,V<sub>T</sub>\`,S\`,P\`) can 
be obtained from G as follows:

1. V<sub>N</sub>\` = V<sub>N</sub> &cup; {S\`}.
2. V<sub>T</sub>\` = V<sub>T</sub> &cup; { $ }.
3. S\` = new starting point.
4. P\` = P &cup; {S\`-->S$}

For example :

> E --> E + T | T
>
> T --> T * F | F
>
> F --> (E) | a

Becomes :

> G --> E$
>
> E --> E + T | T
>
> T --> T * F | F
>
> F --> (E) | a 

This is because we want to create a FOLLOW() set for S.

Lets take another example :

> S\` --> S$
>
> S --> AB
>
> A --> a | &lambda;
>
> B --> b | &lambda;

Lets compute the FIRST() sets for this grammar :

> FIRST(A) = \{a,&lambda;\}
>
> FIRST(B) = \{b,&lambda;\}
> 
> FIRST(S) = FIRST(AB) = FIRST(FIRST(A).FIRST(B))
>
> = FIRST(\{a,&lambda;\},\{b,&lambda;\})
>
> = FIRST(\{a,&lambda;,b,&lambda;})
>
> = \{a,b,&lambda;\}
>
> FIRST(S\`) = FIRST(S$) = FIRST(FIRST(S).FIRST($)) 
>
> = FIRST(\{a,b,&lambda;\}.$)= FIRST(a$,b$,$)
>
> = {a,b,$}

Now Lets compute the FOLLOW() sets for this grammar :

> FOLLOW(S) = \{$\}
>
> FOLLOW(A) = \{b,$\}
>
> FOLLOW(B) = \{$\}

---

Lets Take another, slightly more complex example :

> S\` --> S$
>
> S --> aAcb
>
> S --> Abc
>
> A --> b | c | &lambda;

Lets take the FIRST() for this grammar :

> FIRST(A) = \{b,c,&lambda;\}
>
> FIRST(S) =  FIRST(aAcb)&cup;FIRST(Abc) = \{a,\} &cup \{b,c\}
>
> = \{a,b,c\}
>
> FIRST(S\`) = FIRST(S$) = FIRST(FIRST(S).FIRST($)) 
>
>= FIRST(\{a,b,c\}.\{$\})
>
> = \{a,b,c\}

Now lets take the FOLLOW() : 

> FOLLOW(S) = \{$\}
>
> FOLLOW(A = \{c,b\}

---

Say we have the grammar 

> G --> E$
>
> E --> E + T | T
>
> T --> T * F | F
>
> F --> (E) | a 

Lets calculate FIRST() :

> FIRST(F) = {(,a}
>
> FIRST(T) = FIRST(T * F)&cup;FIRST(F) =  FIRST(T * F)&cup;{\(,a} 
>
> = {\(,a} (Because every T will eventually become an F)
>
> FIRST(E) = FIRST(E + T) &cup; FIRST(T) = {\(,a} &cup; {\(,a} 
>
> = {\(,a} 
>
> FIRST(G) = FIRST(E$) = {\(,a} 

Now lets Calculate FOLLOW() :

> FOLLOW(E) = \{$,+,)\}
>
> FOLLOW(T) = FOLLOW(E) &cup; \{*\} = \{$,+,\*,)\}
>
> FOLLOW(F) = FOLLOW(T) = \{$,+,\*,)\}

---

But what makes all this so important? 

Well, All of the parsing techniques we are going to learn will heavily rely 
on FIRST() and FOLLOW().

---

### Top-Down Parsing (Continued)

#### Recursive Descent Parsing

Recursive Descent Parsing is very simple. It works like this :

1. Divide the grammar into primitive/simple components
	
   1. For the token "a" :
	   
	   ```
	   
	   If(token == "a"){
	   
	   		get-next()
	   
	   }
	   
	   else{
	   
	   	report-error()
	   
	   }
	   
       ```
       
    2.  For X --> &alpha;<sub>1</sub>&alpha;<sub>2</sub>,...,&alpha;<sub>n</sub> :
		
	    ```
	    
		code(X):{ 
		
		code(α1);
		
		code(α2);
		...
		
		...
		
		...
		
		code(αn);
		}
		
		```
		That is, if a production is made of multiple sub productions in order, 
		they must be called in order.
		
		If FIRST(&alpha;<sub>n</sub>) of the production is non-empty, and 
		&alpha;<sub>n</sub> &ne; &lambda;, ie, the production is not 
		an empty one, then
		
		
		``` 
		If(token ∈ FIRST(α1)){
			code(α1)
		}
		 else If(token ∈ FIRST(α2)){
			code(α2)
		}
		...
		
		...
		
		...
		else If(token ∈ FIRST(αn)){
			code(αn)
		}
		else{
		 report-error();
		}
		```
		
		Furthermore, the code for x = &alpha;\*, where we can have
		zero or more consecutive repetitions of a production, we say
		
		```
		while(token ∈ FIRST(α*)){
		
		 call(α*);
		
		 }
		```
---

Notes :

1. Every nonterminal has a code(a function).
2. S\` in augmented grammar is represented by the function "main".
3. We only start with calling "get-token" in function "main".

---

For Example, lets say we have

> G --> E$
>
> E --> T( + T )*
>
> T --> F( \* F )\*
>
> F --> ( E ) | a

Then we can say :

```

 main(){//represents G

 get-token;

 call E();

 if(token!="$")
 {
	Error;
 }
 else{
	SUCCESS;
 }
	
	

}

 function E(){//Represents E -- T (+ T)*

	call T();

	while(token == "+"){

		get-token();

		call T()
	}

 }
 
 function T(){//T--> F (* F)*
 
	call F();
	while(token == "*"){
	
		get-token();
	
		call F();
	}
 }
 
 function F(){
 
	if(token == "(")
	{
		get-token();
		
		call E();
		
		if(token == ")")
		{
			get-token();
		}
		else
		{
			ERROR;
		}
	}
	else if(token=="a")
		{
		get-token();
		}
	else
	{
		ERROR;
	}
 }
 
```

Note that ```ERROR``` is a function we should write.

---

Lets take another example now. 

Given the grammar :

> Program --> body .
> 
> body --> Begin stmt (; stmt)* End
>
> stmt --> Read | Write | body | &lambda;

where we will represent &lambda; as ```l``` from now on in the example;

and

> V<sub>N</sub> = { Program, body, stmt, block}
>
> V<sub>T</sub> = { ., Begin, ;, End, Read, Write}

examples of programs of this language would be

``` 

Begin
	Read;
	Write;
	Read;
	Write;
End.

```
or

```

Begin
	Read;
End.

```

or

```

Begin
	Read;
	Begin
		Read;
		Write;
	End.
	Write;
End.

```

or

```

Begin;
	;
	;
	;
	;
End.

```

Lets write the code for this programming language.

```
main(){

	get-token();
	
	call body();
	
	if(token != "."){
		ERROR;
	}
	else{
		SUCCESS;
	}
}

function body(){

	call Begin();
	if(token == "Begin"){
		get-token();
		call stmt();
		while(token !=";"){
			get-token();
			call stmt();
		}
		if(token == "End"){
			get-token();
		}
		else{
			ERROR;
		}
	
	}
	else{
		ERROR;
	}
}

function stmt(){

	if(token == "Read"){
		get-token();	
	}
	else if(token == "Write"){
		get-token();
	}
	else if(token == "Begin"){
		call body();
	}
	else if(token != ";" || token != "End" ){
		ERROR();
	}
}
		
```

#### LL(1) Parsing

This Parsing method is a **table-driven** parsing method. The LL(1) parsing
table selects which production to choose for the next derivation step. 

##### Formal Definition of LL(1)

The Formal definition of LL(1) grammars is given by :

> Given the Productions :
>
> A --> &alpha;<sub>1</sub>
>
> A --> &alpha;<sub>2</sub>
>
> A --> &alpha;<sub>3</sub>
>
> A --> &alpha;<sub>n</sub>
>
> then the grammar is LL(1) if :
>
> 1. FIRST(&alpha;<sub>i</sub>)&cap;FIRST(&alpha;<sub>g</sub>) = &empty; for all i,j
>
> 1. if one of &alpha;<sub>i</sub>  is &lambda;,&alpha;<sub>n</sub> = &lambda;, in addition to 1, &forall; i &lt; n

For example, Given the grammar :

> S --> S$
>
> S --> aABC
> 
> A --> a | bbD
>
> B --> a | &lambda;
>
> C --> b | &lambda;
>
> D --> C | &lambda;

let us see if it is LL(1)

- FIRST(a)&cap;FIRST(bbD) = &empty;
- FIRST(a)&cap;FOLLOW(B) = &empty;
- FIRST(b)&cap;FOLLOW\(C\) = &empty;
- FIRST\(c\)&cap;FOLLOW(D) = &empty;

Then this grammar is LL(1).

Given another Grammar :

> S` --> S$
>
> S --> aAa | &lambda;
>
> A --> abS | &lambda;

- FIRST(aAa)&cap;FOLLOW(S) = {a}&cap;{$,a} = {a} &ne; &empty;

This grammer is **not** LL(1).

---

Lets assume that we have a grammer that is LL(1). How do we build the LL(1) parsing table?

##### LL(1) Parsing Table Building Algorithm

1. For each production A --> &alpha; in the grammar G, 
   1. Add to the table entry T[A,a] the production A --> &alpha;, where A &isin; FIRST(&alpha;)
2. If &lambda; &isin; FIRST(&alpha;), Add to the table entry T[A,b] the production A --> &alpha; &forall; b &isin; FOLLOW(A).
3. All Remaining Entries are Error Entries.


For example, given the grammar :

>V --> SR $<sup>1</sup>
>
>S --> +<sup>2</sup> | -<sup>3</sup> | &lambda;<sup>4</sup>
>
>R --> dN.N<sup>5</sup> | .dN<sup>6</sup>
>
>N --> dN<sup>7</sup>| &lambda;<sup>8</sup>

note that the superscript denotes the production number.

Then the table will look like this 

> FIRST(SR $) = {+,-,d,.}
> 
> FIRST(+) = {+}
>
> FOLLOW(S) = {d,.}
>
> FIRST\(R\) = {d, .}
>
> FIRST(d) = { d }
>
> FOLLOW(N) = {d, ., $}

V<sub>N</sub>\V<sub>T</sub>| + | - | d | . | $
---|---|---|---|---|---
 V | 1 | 1 | 1 | 1 |
 S | 2 | 3 | 4 | 4 |   
 R |   |   | 5 | 5 |
 N |   |   | 7 | 8 | 8

There should be no conflict(multiple entries) in the LL(1) table.

L(G) of this grammar = all floating point numbers.


The parser works like this

Stack | Remaining Input | Action 
---|---|---|
V    |-dd.d$ | Production 1
SR$  |-dd.d$ | Production 3
-R$  |-dd.d$ | Pop & advance input
R$   | dd.d$ | Production 5
dN.N$| dd.d$ | Pop & advance input
N.N$ |  d.d$ | Production 7
dN.N$|  d.d$ | Pop & advance input
N.N$ |   .d$ | Production 8
.N$  |   .d$ | Pop & advance input
N$   |    d$ | Production 7
dN$  |    d$ | Pop & advance
N$   |     $ | Production 8
$    |     $ | Pop and Advance
&lambda; | &lambda; | Accept 

If at any point the parser reaches a place where the input and the 
stack have 2 different terminal symbols, it throws a syntax error.

---

Lets Take another example. Let the Grammar be :

> program --> block $ <sup>1</sup>
>
> block --> { declarations stmnts } <sup>2</sup>
>
> decls --> D ; decls <sup>3</sup> | &lambda; <sup>4</sup>
>
> stmnts --> statement ; stmts <sup>5</sup> | &lambda; <sup>6</sup>
>
> statement --> if <sup>7</sup> | while <sup>8</sup> | ass <sup>9</sup> | scan <sup>10</sup> | print<sup>11</sup> | block <sup>12</sup> | &lambda;<sup>13</sup>

V<sub>T</sub> = {$,{,},D,;,if,while,ass,scan,print}


V<sub>N</sub>\V<sub>T</sub>|if|while|ass|scan|print|\{|\}|D|;|$
---|---|---|---|---|---|---|---|---|---|---
Program  |   |   |   |   |   | 1 |   |   |   |   
block    |   |   |   |   |   | 2 |   |   |   |   
decls    | 4 | 4 | 4 | 4 | 4 | 4 | 4 | 3 | 4 |   
stmts    | 5 | 5 | 5 | 5 | 5 | 5 | 6 |   | 5 |   
statement| 7 | 8 | 9 | 10| 11| 2 |   |   | 13|   

No conflict. 

---

Another example is  the If....else statement with a delimiter. the grammar 
looks like this :

> S` --> S$
>
> S --> iCSE
>
> E --> S | &lambda;
>
> S --> a
>
> C --> c

V<sub>N</sub>\V<sub>T</sub>| i | a | e | c | $
---|---|---|---|---|---
S\`| 1 | 1 |   |   |   
S  | 2 | 5 |   |   |   
E  |   |   |3,4|   | 4   
C  |   |   |   | 6 |

There is a conflict. To solve this, we can add a delimiter.

> S` --> S$
>
> S --> iCSEd
>
> E --> eS | &lambda;
>
> S --> a
>
> C --> c

V<sub>N</sub>\V<sub>T</sub>| i | a | e | c | d | $
---|---|---|---|---|---|---
S\`| 1 | 1 |   |   |   |
S  | 2 | 5 |   |   |   |
E  |   |   | 3 |   | 4 | 
C  |   |   |   | 6 |   |

The grammar is now unambiguous. Alternatively, we can just strike out
the transition 4 from the LL(1) table, which is a &lambda; transition.
Lets follow through the derivation tree. the resultant table is 

V<sub>N</sub>\V<sub>T</sub>| i | a | e | c | $
---|---|---|---|---|---
S\`| 1 | 1 |   |   |   
S  | 2 | 5 |   |   |   
E  |   |   | 3 |   | 4   
C  |   |   |   | 6 |

and this works because the natural behavior of the else-part is to follow
the nearest if statement.

---

Something Important to note is that **if a grammar is LL(1), then it is 
unambiguous. However, the opposite is not necessarily true.**

Another thing to note is that 
in Top-Down parsing, we should avoid a grammar that is not LL(1). 

--- 

### Bottom-Up Parsing (Continued)

Recall that in Bottom-Up parsing, the parser starts from the given sentence,
applying reductions until it reaches the starting symbol of the grammar or 
a deadlock. The major problem with Bottom-Up parsing is which substring
we should select in each reduction step.

The answer to the above question is : In each reduction step, we select 
what is called **the handle**.

The Handle is obtained by a **rightmost** derivation **in reverse** 

For example, Given the grammar :

>V --> S R $
>
>S --> +|-|&lambda;
>
>R --> .dN | dN.N
>
>N --> dN | &lambda;

and the sentence

> -dd.d$

First, we derive the sentence rightmost. 

> V --<sup>rm</sup>--> SR$ --<sup>rm</sup>--> SdN.N$ --<sup>rm</sup>--> SdN.dN$ --<sup>rm</sup>--> SdN.dd$ --<sup>rm</sup>--> SddN.d$
> --<sup>rm</sup>--> Sdd.d$ --<sup>rm</sup>--> -dd.d$

So our handles would be :

> V <-- **SR$** <-- S**dN.N**$<--
> SdN.**dN**$ <-- SdN.d**&lambda;**$ <-- Sd**dN**.d$ <-- Sdd**&lambda;**.d$ <-- **-**dd.d$

But Compilers dont work like this. We already derived the sentence, why would we go 
back and do it again?

We cannot build a Bottom-Up parser for every Context-Free Grammar. However,
we are fortunate enough that there exist subsets of the Context-Free Grammar 
for which we can build a deterministic Bottom-Up parser ie, the parser
can determine/decide precisely where the handle is in each reduction step.

some  of these subsets are 

- LR Parsers :
  - SLR(Simple-LR).
  - LALR(Look-Ahead LR).
  - LR.

- Operator Precedence.

We will only be talking about the SLR parser, just to get an idea
of how Bottom-Up parsing works.

#### SLR Parsing

SLR parsing, and LR parsing in general, is a tabular parsing method.

All LL(1) grammars are a subset of SLR grammars. 

All LR parsers contains :

1. A parsing table.

2. A stack.

3. The input string.

As a reminder, the LL(1) parser contains :


1. A parsing table.

2. A stack.

3. The input string.

However, the way we build it is different.

##### Building the SLR Parsing Table

> An LR(0) item of a grammar G is a production in G with a dot(.) at
> some position in the right side.

For example, the production 

> A --> aBY 

This production generates the following LR(0) items :

> A --> .aBY
>
> A --> a.BY
>
> A --> aB.Y
>
> A --> aBY. --> complete item

Note that for A --> &lambda;, this generates only A --> &lambda;. .

Generally speaking, if the right side of the production is L, then 
there are L+1 resultant LR(0) items.

The LR(0) item 

> A --> aB.Y

Means that the parser has scanned on the input a string derived from 
aB and expects to see a string derived from Y .

We need to define the following 2 functions. 

##### The CLOSURE function 

``` 

function CLOSURE(I)//I is a set of LR(0)items
{
	Repeat 
		for(every LR(0) item A-->α.Bβ in I,
		and for every production B-->γ in G,
		Add the LR(0)item B-->.γ to I)
	Until no more items to be added;
}

```

Lets apply this to our grammar :

> (1)E --> E+T 
> 
> (2)E --> T 
>
> (3)T --> T*F
>
> (4)T --> F
>
> (5)F --> (E) 
>
> (6)F --> a

This grammar is not LL(1) because

> FIRST(T*F)&cap;FIRST(F) = {(,a} &ne; &empty;

We will need to build the LR(0) sets of items.

we start with I<sub>0</sub

> I<sub>0</sub>: E` --> .E
>
> CLOSURE(I<sub>0</sub>)
>
> E` --> &lambda;.E&lambda;
>
> A ---> &gamma;.B&beta;
>

we add all productions starting with E and add the . at the start
therefore :

>I<sub>0</sub>:{E1-->.E, E-->.E+T, E-->.T}

Lets look at  E-->.E+T

>  E-->&lambda;.E+T
>
>  A ---> &gamma;.B&beta;

Therefore, we add all productions starting wtih T to I<sub>0</sub>

> I<sub>0</sub>:{E1-->.E, E-->.E+T, E-->.T, T--> .T*F, T-->.F}
>

we iterate again, and the resultant set is:

> I<sub>0</sub>:{E`-->.E, E-->.E+T, E-->.T, T--> .T*F, T-->.F, F-->.(E), F-->.a}
>

##### The GOTO function

```

function GOTO(I,X)//I=set of items,X=Grammar symbol
{

   CLOSURE(all items A-->αX.β Where A-->α.Xβ in I)

}

```

Lets apply this to the grammar above. First we seperate 
each grammar symbol production to its own set This results in 4 Item groups:

> I<sub>1</sub> : E-->.E, E-->.E+T
>
>I<sub>2</sub> : E-->.T, T--> .T*F
>
>I<sub>3</sub> : T-->.F
>
>I<sub>4</sub> : F-->.(E)
>
> I<sub>5</sub> :F-->.a

and take the CLOSURE for all these sets. The resultant is :

> I<sub>1</sub> : E-->E., E-->E.+T
>
> I<sub>2</sub> : E-->T., T--> T.*F
>
> I<sub>3</sub> : T-->F.
>
> I<sub>4</sub> : F-->(.E), E --> .E+T,E-->.T,E-->.T*F,T-->.F,F-->.(E),F-->.a
>
> I<sub>5</sub> : F-->.a
>
> I<sub>6</sub> : E-->E+.T,E-->.T*F,T-->.F,F-->.(E),F-->.a
>
> I<sub>7</sub> : E-->T*.F,F-->.(E),F-->.a
>
> I<sub>8</sub> : F-->(E.),E-->E.+T
>
> I<sub>9</sub> : E --> E+T. , T --> T.*F
>
> I<sub>10</sub> : T --> T*F.
>
> I<sub>11</sub> : F --> (E).


##### Constructing the SLR table

Input : LR(0) sets of items

Output : SLR(1) parsing table

1. For every item A-->&alpha;.A&beta; in I<sub>i</sub>, &alpha; &isin; V<sub>T</sub> , and
 GOTO(I<sub>i</sub>,a)=I<sub>j</sub>, then set; then ACTION[i,a]=S<sub>j</sub>(shift and push j on the stack).
 
2. For item A-->&alpha;.(complete item) in I<sub>i</sub>, ACTION[i,b]=Reduce by a-->&Alpha; FOR ALL b &isin; FOLLOW(A).

3. For S` --> S. in I<sub>i</sub>, ACTION[i,$] = Accept.

4. If GOTO(I<sub>i</sub>,A) = I<sub>j</sub> then set, GOTO(i,A) = j.

5. All remaining entries are error entries.


lets apply this to the example above and generate the table

V  |<sup>Action</sup>  | a | + | * | ( | ) | $ |<sup>GOTO</sup>  | E | T | F
---|---|---|---|---|---|---|---|---|---|---|---
0  |   |S5 |   |   |S4 |   |   |   | 1 | 2 | 3
1  |   |   |S6 |   |   |   | A |   |   |   |
2  |   |   |R2 |S7 |   |R2 |R2 |   |   |   |
3  |   |   |R4 |R4 |   |R4 |R4 |   |   |   |
4  |   |S5 |   |   |S4 |   |   |   | 8 | 2 | 3
5  |   |   |R6 |R6 |   |R6 |R6 |   |   |   | 
6  |   |S5 |   |   |S4 |   |   |   |   | 9 | 3
7  |   |S5 |   |   |S4 |   |   |   |   |   | 10
8  |   |   |S6 |   |   |S11|   |   |   |   |
9  |   |   |R1 |   |S7 |R1 |R1 |   |   |   |
10 |   |   |R3 |R3 |   |R3 |R3 |   |   |   |
11 |   |   |R5 |R5 |   |R5 |R5 |   |   |   |


No conflict --> SLR(1) grammar

##### Parsing The SLR Table

> E --> E+T 
> 
> E --> T 
>
> T --> T*F
>
> T --> F
>
> F --> (E) 
>
> F --> a

Lets examine the sentence 

> a + a $


Stack         | Remaining | Action
--------------|-----------|-------
0             | a + a $   | S5
0 a 5         |   + a $   | R6
0 F 3         |   + a $   | R4
0 T 2         |   + a $   | R2
0 E 1         |   + a $   | S6
0 E 1 + 6     |     a $   | S5
0 E 1 + 6 a 5 |     $     | R6
0 E 1 + 6 F 3 |     $     | R4
0 E 1 + 6 T 9 |     $     | R1
0 E 1         |     $     | Accept

---

But what if the grammar is not SLR?

#### LR Parsing Techniques

The main difference between LR and SLR is the CLOSURE function

``` 

function CLOSURE(I)//I is a set of LR(1)items
{
	Repeat 
		for(every LR(1) item [A-->α.Bβ, a] in I,
		and for every production B-->γ in G,
		Add the LR(1)item [B-->.γ , b] where b belongs to FIRST(β a)to I)
	Until no more items to be added;
}

```

Where An LR(1) item is an LR(0) item with a **Lookahead Symbol**. For example

[A --> &alpha;.&beta; , a] where a is the lookahead. The lookahead symbol
"a" has no effect whatsoever on an item [A --> &alpha;.&beta;,a] &beta; &ne; &lambda; (not complete item) However,
if the item is a complete [A --> &alpha;.,a], this means we reduce by the 
production A --> &alpha; on token "a".

For example, Give the grammar :

> S --> S`
>
> S --> CC (1)
>
> C --> cC (2)
>
> C --> d (3)

I<sub>0</sub>:

- S` --> .S, $ I<sub>1</sub>


- S --> .CC, FIRST(&lambda;$) = FIRST($) = $ I<sub>2</sub>


- C --> .cC, FIRST(C$) = c,d I<sub>3</sub>


- C --> .d,   c,d I<sub>4</sub>

I<sub>1</sub>:

- S` --> .S, $ Accept

I<sub>2</sub>:

- S --> C.C , $  I<sub>5</sub> (if the lookahead is different it goes to a new state)
- C --> .cC , $  I<sub>6</sub>
- C --> .d , $  I<sub>7</sub>


I<sub>3</sub>:

- C --> c.C , c,d I<sub>8</sub>
- C --> .cC   c,d I<sub>3</sub>
- C --> .d I<sub>4</sub>

I<sub>4</sub>:

- C --> d.,   c,d Complete

I<sub>5</sub> :

- S --> CC. , $ Complete

I<sub>6</sub> :

- C --> c.C , $ I<sub>9</sub>
- C --> .cC , $ I<sub>6</sub>
- C --> .d , $ I<sub>7</sub>

I<sub>7</sub> :

- C --> d. , $ Complete

I<sub>8</sub> :

- C --> cC. , c,d Complete

I<sub>9</sub> :

- C --> cC. , $ Complete


V  |<sup>Action</sup>  | c | d | $ |<sup>GOTO</sup> | S | C
---|-------------------|---|---|---|----------------|---|---
0  |                   |S3 |S4 |   |                | 1 | 2 
1  |                   |   |   | A |                |   |   
2  |                   |S6 |S7 |   |                |   | 5 
3  |                   |S3 |S4 |   |                |   | 8  
4  |                   |R3 |R3 |   |                |   |   
5  |                   |   |   |R1 |                |   |   
6  |                   |S6 |S7 |   |                |   | 9  
7  |                   |   |   |R3 |                |   |   
8  |                   |R2 |R2 |   |                |   |   
9  |                   |   |   |R2 |                |   |   

---

No conflict, the grammar is an LR grammar.

---

If we look at the above example, we can see that some sets of items have the 
same core items(LR(0) items), but the lookahead is different. For example (I<sub>7</sub> , I<sub>4</sub>),
(I<sub>3</sub> , I<sub>6</sub>), (I<sub>8</sub> , I<sub>9</sub>). Lets say we 
merge the states. What happens is the we add the lookaheads and 
change the state references for equivalent states . The table becomes this:

V  |<sup>Action</sup>  | c | d | $ |<sup>GOTO</sup> | S | C
---|-------------------|---|---|---|----------------|---|---
0  |                   |S3 |S4 |   |                | 1 | 2 
1  |                   |   |   | A |                |   |   
2  |                   |S3 |S4 |   |                |   | 5 
3  |                   |S3 |S4 |   |                |   | 8  
4  |                   |R3 |R3 |R3 |                |   |   
5  |                   |   |   |R1 |                |   |   
8  |                   |R2 |R2 |R2 |                |   |     

---

This is now a simplified table. if the parsing table after
merging has no conflicts(like in the above example), then 
the grammar is an LALR(1) Grammar. 
